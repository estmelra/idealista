{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Idealista scraper**\n",
    "*Jose Ramon Estevez Melgarejo 2022-03-04.*\n",
    "\n",
    "## Introduction\n",
    "Wellcome to the first mini project of my personal portfolio where I create a web scraper to obtain valuable informmation from [Idealista](https://www.idealista.com). When structuring my portfolio I thought that a good Exploratory Data Analysis (EDA) was the first thing I needed to do in order to showcase my abilities. After searching for a messy dataset I could work with for some time I decided to create my own scraper to obtain data for my analysis. So this project will be divided into tree parts:\n",
    "\n",
    "1.  Data extraction via web scraping (current notebook) \n",
    "2.  Exploratory Data Analysis (EDA)\n",
    "3.  Machine Learning Regression model to estimate huse price\n",
    "\n",
    "### Why idealista?\n",
    "Well idealista is one of the main real state platforms that spanish people use to sell and buy houses. After having worked as a data scientist for more than three years I have finished plenty of projects but it is difficult to use them for my portfolio whithout revealing sensitive informmation about the companies I have been working for. Therefore, I decided tu develop a study about the real state situation at my home city (Cadiz) that I could maybe benefit from. Also, idealista web site is a relitively easy site to scrap and data is not really well structured (I wanted to start my EDA with messy data as data cleanishing is also a big part of data science).\n",
    "\n",
    "\n",
    "### Scraping method\n",
    "As far as I know there are three main scraping libraries / frameworks. Scrapy, Selenium and Requests + Beautifulsoup. Eventhough Scrapy and selenium seem to be more robust alternatives I decided not to overcomplicate  my self and use Requests + Beautifulsou given that idealista web site is relatively simple to scrap.\n",
    "\n",
    "## Index\n",
    "1.  Importing Libraries & script genral variables\n",
    "2.  Scraping free proxies and defining Request Headers\n",
    "3.  Houses scraping\n",
    "    1.  Houses ids scraping\n",
    "    2.  Houses info scraping\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import proxies_scr # source to a personal script to scrap free proxy sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define two variables that will help us to limit our house search if we wanted to and select the number of free working proxies that  we will use to scrpa our data. why we use free proxies will be explained in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_limit = 10 # Limit search of houses.\n",
    "n_prox = 7 # Selecting the number of free proxies to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Scraping free proxies and defining Request Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very common that websites block your IP if you do many consecutive requests in a short period of time or if they suspect that one entering the site is not a human directly (our case). So in order to avoid been blocked we have to do two things. prueba git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to send headers so that idealista recognises us as a person and not a bot\n",
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'es,es-ES;q=0.9,en;q=0.8,fr;q=0.7',\n",
    "    'cache-control': 'no-cache',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': 'https://www.idealista.com/en/areas/venta-viviendas/?shape=%28%28ez_%7EEn%7Bse%40_bCceIniKcdFpvAfiEa%7EI%7E_J%29%29',\n",
    "    #'sec-ch-ua': \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"98\", \"Google Chrome\";v=\"98\",\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': 'macO',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36',\n",
    "    'x-newrelic-id': 'VQIGUlZbGwIBXFhWBQEDVw==',\n",
    "    'x-requested-with': 'XMLHttpRequest'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting working proxies.\n",
      "total fail\n",
      "total fail\n",
      "total fail\n",
      "total fail\n",
      "429\n",
      "dldcalksmnc\n",
      "1 proxies found.\n"
     ]
    }
   ],
   "source": [
    "# Looking  for proxies\n",
    "print('Selecting working proxies.')\n",
    "working_proxies = []\n",
    "tested_proxies = []\n",
    "round = 1\n",
    "n_prox = 1\n",
    "\n",
    "while len(working_proxies) < n_prox:\n",
    "    print(f'Round {round} of proxy scrapping.')\n",
    "    print(f'{len(tested_proxies)} proxies tested')\n",
    "    print(f'{len(working_proxies)} proxies found')\n",
    "    proxies = proxies_scr.get_proxies()\n",
    "    for prox in proxies:\n",
    "        if prox not in working_proxies and prox not in tested_proxies:\n",
    "            if len(working_proxies) >= n_prox:\n",
    "                break\n",
    "            else:\n",
    "\n",
    "                try:\n",
    "                    url = 'https://www.idealista.com/'\n",
    "                    proxy = 'http://' + prox\n",
    "                    r = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy},  timeout=10)\n",
    "                    print(r.status_code)\n",
    "                    if r.status_code in [200, 429]:\n",
    "                        working_proxies.append(prox)\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        tested_proxies.append(prox)\n",
    "                except:\n",
    "                    tested_proxies.append(prox)\n",
    "\n",
    "    if len(working_proxies) < n_prox:\n",
    "        time.sleep(20)\n",
    "\n",
    "    round = round + 1\n",
    "    \n",
    "\n",
    "print(f'{len(working_proxies)} proxies found.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://www.idealista.com/'\n",
    "prox = ''\n",
    "if prox != '':\n",
    "    proxy = 'http://' \n",
    "else:\n",
    "    proxy = ''\n",
    "\n",
    "r = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy},  timeout=10)\n",
    "r.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'sdf' is not defined\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be3ee27bd6789846cabdaaf1d172d1b46fe3799ccedac2ef9c2fd3b8a24e7656"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
