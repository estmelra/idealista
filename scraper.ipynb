{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Idealista scraper**\n",
    "*Jose Ramon Estevez Melgarejo 2022-03-04.*\n",
    "\n",
    "## Introduction\n",
    "Welcome to the first mini project of my personal portfolio where I create a web scraper to obtain valuable information from [Idealista](https://www.idealista.com). When structuring my portfolio I thought that a good Exploratory Data Analysis (EDA) was the first thing I needed to do in order to showcase my abilities. After searching for a messy dataset I could work with for some time I decided to create my own scraper to obtain data for my analysis. So this project will be divided into tree parts:\n",
    "\n",
    "1.  Data extraction via web scraping (current notebook) \n",
    "2.  Exploratory Data Analysis (EDA)\n",
    "3.  Machine Learning Regression model to estimate house price\n",
    "\n",
    "### Why idealista?\n",
    "Well idealista is one of the main real state platforms that Spanish people use to sell and buy houses. After having worked as a data scientist for more than three years I have finished plenty of projects but it is difficult to use them for my portfolio without revealing sensitive information about the companies I have been working for. Therefore, I decided to develop a study about the real state situation at my home city (Cadiz) that I could maybe benefit from. Also, idealista website is a relatively easy site to scrap and data is not really well-structured (I wanted to start my EDA with messy data as data cleansing is also a big part of data science).\n",
    "\n",
    "\n",
    "### Scraping method\n",
    "As far as I know there are three main scraping libraries / frameworks. Scrapy, Selenium and Requests + Beautiful soup. Even though Scrapy and selenium seem to be more robust alternatives I decided not to over complicate my self and use Requests + Beautiful sou given that idealista website is relatively simple to scrap.\n",
    "\n",
    "## Index\n",
    "1.  Importing Libraries & script general variables\n",
    "2.  Scraping free proxies and defining Request Headers\n",
    "3.  Houses scraping\n",
    "    1.  Houses ids scraping\n",
    "    2.  Houses info scraping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import proxies_scr # source to a personal script to scrap free proxy sites\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will define two variables that will help us to limit our house search if we wanted to and select the number of free working proxies that we will use to scrap our data. Why we use free proxies will be explained in the next section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_limit = 10 # Limit search of houses.\n",
    "n_prox = 1 # Selecting the number of free proxies to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Scraping free proxies and defining Request Headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " It is very common that websites block your IP if you do many consecutive requests in a short period of time or if they suspect that the one entering the site is not a human (our case). So in order to avoid been blocked we have to do two things. \n",
    "\n",
    "-  First, we need to send some request headers. Request headers provide information about the request context. Without headers, we will have a 403 response meaning that our access is denied.\n",
    "\n",
    "-  Second, we need to rotate proxies. As mentioned before if we make too many requests in a short period of time, idealista will block our IP. A way around this is to use some free proxies and alternate IPs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Request Headers\n",
    "\n",
    "This point is simple to solve, you just need to inspect the website, under the Network section you will find the request headers which you can copy and format as a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'accept': '*/*',\n",
    "    'accept-encoding': 'gzip, deflate, br',\n",
    "    'accept-language': 'es,es-ES;q=0.9,en;q=0.8,fr;q=0.7',\n",
    "    'cache-control': 'no-cache',\n",
    "    'pragma': 'no-cache',\n",
    "    'referer': 'https://www.idealista.com/en/areas/venta-viviendas/?shape=%28%28ez_%7EEn%7Bse%40_bCceIniKcdFpvAfiEa%7EI%7E_J%29%29',\n",
    "    #'sec-ch-ua': \" Not A;Brand\";v=\"99\", \"Chromium\";v=\"98\", \"Google Chrome\";v=\"98\",\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': 'macO',\n",
    "    'sec-fetch-dest': 'empty',\n",
    "    'sec-fetch-mode': 'cors',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36',\n",
    "    'x-newrelic-id': 'VQIGUlZbGwIBXFhWBQEDVw==',\n",
    "    'x-requested-with': 'XMLHttpRequest'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Searching for free proxies\n",
    "\n",
    "This is by itself another easy mini scraping project. \n",
    "The function 'get_proxies()' located in 'proxies_scr.py' will scrap two websites where we can find lists of free proxies (https://www.proxy-list.download/HTTP & https://free-proxy-list.net/). This is very convenient but there is a little problem, idealista sees these lists the same way we do, and they try to block them. This means that not all proxies will work. In order to make the whole process a bit faster, we will first try proxies to see if they work. \n",
    "\n",
    "The purpose of this while loop is to keep trying proxies until we have a considerable amount of them. The amount of working proxies can be decided setting the variable 'n_prox' with the number we want (Section 1). After the while loop we append ' ' to the pool of proxies to account with our local IP as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting working proxies.\n",
      "1 proxies found plus local one.\n",
      "working proxies are:\n",
      "['188.138.106.158:5566', '']\n"
     ]
    }
   ],
   "source": [
    "# Looking  for proxies\n",
    "print('Selecting working proxies.')\n",
    "working_proxies = []\n",
    "tested_proxies = []\n",
    "round = 1\n",
    "\n",
    "while len(working_proxies) < n_prox:\n",
    "    #print(f'Round {round} of proxy scrapping.')\n",
    "    #print(f'{len(tested_proxies)} proxies tested')\n",
    "    #print(f'{len(working_proxies)} proxies found')\n",
    "    proxies = proxies_scr.get_proxies()\n",
    "    for prox in proxies:\n",
    "        if prox not in working_proxies and prox not in tested_proxies:\n",
    "            if len(working_proxies) >= n_prox:\n",
    "                break\n",
    "            else:\n",
    "\n",
    "                try:\n",
    "                    url = 'https://www.idealista.com/'\n",
    "                    proxy = 'http://' + prox\n",
    "                    r = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy},  timeout=10)\n",
    "                    #print(r.status_code)\n",
    "                    if r.status_code in [200, 429]:\n",
    "                        working_proxies.append(prox)\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        tested_proxies.append(prox)\n",
    "                except:\n",
    "                    tested_proxies.append(prox)\n",
    "\n",
    "    if len(working_proxies) < n_prox:\n",
    "        time.sleep(20)\n",
    "\n",
    "    round = round + 1\n",
    "    \n",
    "\n",
    "print(f'{len(working_proxies)} proxies found plus local one.')\n",
    "working_proxies.append('') # to add local IP\n",
    "print('working proxies are:')\n",
    "print(working_proxies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Houses scraping\n",
    "Now that we have our request headers and our pool of working proxies we can start scraping houses information from idealista.\n",
    "\n",
    "At idealista.com we have the option to select a searching area. For this study I have selected the area that I am interested in which is Cadiz city (the most beautiful city in Spain, my home city). \n",
    "After selecting the area of interest we get an url ('https://www.idealista.com/en/areas/venta-viviendas/pagina-{x}?shape=%28%28ez_%7EEn%7Bse%40_bCceIniKcdFpvAfiEa%7EI%7E_J%29%29' where 'x' of 'pagina-{x}' is the page number of the results) that we can use to scrap all houses ids posted for that area in all result pages. After obtaining all houses ids we can proceed to request information from each house url.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Houses ids scraping\n",
    "With the following chunk of code we will scrap all ids for the search area but notice that we created a variable in section 1 to limit the amount of houses ids scraped. \n",
    "\n",
    "We will use the pool of free proxies found and request headers to make our requests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding houses ids\n",
      "Houses Ids found:\n",
      "['94283285', '89113690', '95291210', '96746079', '96622572', '94971680', '84849222', '84635808', '95854107', '93809726']\n"
     ]
    }
   ],
   "source": [
    "print('Finding houses ids')\n",
    "x = 1 # for pagination\n",
    "ids = []\n",
    "while True:\n",
    "    if len(ids) >= search_limit:\n",
    "        #print(f'{len(ids)} ids scraped')\n",
    "        #print('Limit reached')\n",
    "        break\n",
    "\n",
    "    url = f'https://www.idealista.com/en/areas/venta-viviendas/pagina-{x}?shape=%28%28ez_%7EEn%7Bse%40_bCceIniKcdFpvAfiEa%7EI%7E_J%29%29'\n",
    "    random_index_prox = random.randint(0, len(working_proxies)-1)\n",
    "    prox = working_proxies[random_index_prox]\n",
    "    if prox != '':\n",
    "        proxy = 'http://' + prox\n",
    "    else:\n",
    "        proxy = ''\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers, proxies={'http': proxy, 'https': proxy},  timeout=10)\n",
    "        #print('IP: ' + proxy + f' -- status code: {r.status_code}')\n",
    "        soup = bs(r.text, 'html.parser')\n",
    "        selected_page = soup.find('main', {'class':'listing-items'}).find('div', {'class':'pagination'}).find('li', {'class':'selected'}).text\n",
    "\n",
    "        if x == int(selected_page):\n",
    "            articles = soup.find('main', {'class':'listing-items'}).find_all('article')\n",
    "        else:\n",
    "            break\n",
    "                \n",
    "        homes_ids = [article.get('data-adid') for article in articles]\n",
    "                \n",
    "        for id in homes_ids:\n",
    "            if id:\n",
    "                ids.append(id)\n",
    "                \n",
    "        x = x + 1\n",
    "        #print('IP seems to work')\n",
    "        #print(f'{len(ids)} ids scraped')\n",
    "\n",
    "    except:\n",
    "        #print('IP: ' + proxy +  ' -- failed, trying a new one.')\n",
    "        #time.sleep(random.randint(1, 5)*random.random()) # to avoid getting blocked\n",
    "\n",
    "        pass\n",
    "\n",
    "if len(ids) > search_limit:\n",
    "    ids = ids[0:search_limit]\n",
    "    \n",
    "print('Houses Ids found:')\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Houses info scraping\n",
    "Now that we have all our hoses ids we can scrap the information about each house. This process might take long because some of our working proxies might get blocked or removed during the process. There is also the possibility to pay to get more reliable proxies but since this is a small personal project and I have no rush, waiting is not a big deal. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding houses info\n",
      "   house_id                                             tittle   city  \\\n",
      "0  94283285     Flat / apartment for sale in Zona Bahía Blanca  Cadiz   \n",
      "1  89113690  Duplex for sale in Mentidero - Teatro Falla - ...  Cadiz   \n",
      "2  95291210  Flat / apartment for sale in Mentidero - Teatr...  Cadiz   \n",
      "3  96746079  Flat / apartment for sale in Urb. alameda apod...  Cadiz   \n",
      "4  96622572  Flat / apartment for sale in Urb. Playa Santa ...  Cadiz   \n",
      "\n",
      "  price_act price_first                                     details_1_desc  \\\n",
      "0   750,000     750,000  [262 m² built, 6 bedrooms, 4 bathrooms, Terrac...   \n",
      "1   575,000     575,000  [135 m² built, 3 bedrooms, 3 bathrooms, Terrac...   \n",
      "2   320,000     320,000  [190 m² built, 8 bedrooms, 3 bathrooms, Second...   \n",
      "3   390,000     390,000  [131 m² built, 117 m² floor area, 3 bedrooms, ...   \n",
      "4   720,000     720,000  [168 m² built, 167 m² floor area, 3 bedrooms, ...   \n",
      "\n",
      "                   details_2_desc details_3_desc               advertiser  \\\n",
      "0  [Air conditioning, en trámite]             []  Professional advertiser   \n",
      "1                    [en trámite]             []  Professional advertiser   \n",
      "2                    [en trámite]             []  Professional advertiser   \n",
      "3  [Air conditioning, en trámite]             []  Professional advertiser   \n",
      "4                    [en trámite]             []  Professional advertiser   \n",
      "\n",
      "                    datetime  \n",
      "0 2022-03-09 13:33:35.571467  \n",
      "1 2022-03-09 13:33:35.571467  \n",
      "2 2022-03-09 13:33:35.571467  \n",
      "3 2022-03-09 13:33:35.571467  \n",
      "4 2022-03-09 13:33:35.571467  \n"
     ]
    }
   ],
   "source": [
    "# Getting all info from individual houses\n",
    "print('Finding houses info')\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "for id in ids:\n",
    "    house_url = f'https://www.idealista.com/en/inmueble/{id}/'\n",
    "    keep_searching_proxi = True\n",
    "\n",
    "    while keep_searching_proxi:\n",
    "        random_index_prox = random.randint(0, len(working_proxies)-1)\n",
    "        prox = working_proxies[random_index_prox]\n",
    "        if prox != '':\n",
    "            proxy = 'http://' + prox\n",
    "        else:\n",
    "            proxy = ''\n",
    "\n",
    "        try:\n",
    "            r = requests.get(house_url, headers=headers, proxies={'http': proxy, 'https': proxy},  timeout=10)\n",
    "            \n",
    "            if r.status_code == 404:\n",
    "                print('ID: ' + id +  ' -- Remmoved from idealista.')\n",
    "                keep_searching_proxi = False\n",
    "            \n",
    "            elif r.status_code == 200:\n",
    "                tries = 0\n",
    "                while tries < 10:\n",
    "                    try:\n",
    "                        #print('IP:' +  proxy + ' -- seems to work')\n",
    "                        soup = bs(r.text, 'html.parser')\n",
    "                        tittle = soup.find('span', {'class':'main-info__title-main'}).text\n",
    "                        city = soup.find('span', {'class':'main-info__title-minor'}).text\n",
    "                        price_act = soup.find('span', {'class':'info-data-price'}).find('span', {'class':'txt-bold'}).text\n",
    "\n",
    "                        try:\n",
    "                            price_first = soup.find('span', {'class':'pricedown'}).find('span', {'class':'pricedown_price'}).text\n",
    "                        except:\n",
    "                            price_first = price_act\n",
    "\n",
    "                        details_1 = soup.find('div', {'class':'details-property-feature-one'}).find_all('div', {'class':'details-property_features'})\n",
    "                        details_1_desc = []\n",
    "                        for ul in details_1:\n",
    "                            lis = ul.findAll('li')\n",
    "                            for li in lis:\n",
    "                                details_1_desc.append(li.text.strip())\n",
    "\n",
    "                        details_2 = soup.find('div', {'class':'details-property-feature-two'}).find_all('div', {'class':'details-property_features'})\n",
    "                        details_2_desc = []\n",
    "                        for ul in details_2:\n",
    "                            lis = ul.findAll('li')\n",
    "                            for li in lis:\n",
    "                                details_2_desc.append(li.text.strip())\n",
    "\n",
    "                        details_3 = soup.find('div', {'class':'details-property-feature-three'}).find_all('div', {'class':'details-property_features'})\n",
    "                        details_3_desc = []\n",
    "                        for ul in details_3:\n",
    "                            lis = ul.findAll('li')\n",
    "                            for li in lis:\n",
    "                                details_3_desc.append(li.text.strip())\n",
    "\n",
    "                        advertiser = soup.find('div', {'class':'professional-name'}).find('div', {'class':'name'}).text.strip()\n",
    "\n",
    "                        line = {\n",
    "                            'house_id': id,\n",
    "                            'tittle': tittle,\n",
    "                            'city': city,\n",
    "                            'price_act': price_act,\n",
    "                            'price_first': price_first,\n",
    "                            'details_1_desc': details_1_desc,\n",
    "                            'details_2_desc': details_2_desc,\n",
    "                            'details_3_desc': details_3_desc,\n",
    "                            'advertiser' : advertiser\n",
    "                        }\n",
    "\n",
    "                        raw_data = raw_data.append(line, ignore_index=True)\n",
    "                        #time.sleep(random.randint(1, 3)*random.random()) # to avoid getting blocked\n",
    "                        to_go = len(ids) - ids.index(id)\n",
    "                        #print(f'remaining houses to scrap: {to_go}')\n",
    "                        keep_searching_proxi = False\n",
    "                        tries = tries + 10\n",
    "                    except Exception as e: \n",
    "                        #print('Estatus code 200 but not working')\n",
    "                        #print(e)\n",
    "                        tries = tries + 1\n",
    "                        keep_searching_proxi = False\n",
    "            \n",
    "            else:\n",
    "                #print('IP: ' + proxy +  f' -- failed due a status code {r.status_code}, trying a new one.')\n",
    "                pass\n",
    "\n",
    "\n",
    "        except:\n",
    "            #print('IP: ' + proxy +  ' -- failed, trying a new one. General error')\n",
    "            #time.sleep(random.randint(1, 5)*random.random()) \n",
    "            pass\n",
    "\n",
    "raw_data['datetime'] = dt.datetime.now() # adding datetime col to know when we scraped.\n",
    "\n",
    "print(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the field description of our result dataframe:\n",
    "\n",
    "-   ***house_id***: unique house identifier.\n",
    "\n",
    "-   ***tittle***: tittle of the offer\n",
    "\n",
    "-   ***city***: city where the house is located\n",
    "\n",
    "-   ***price_act***: actual price\n",
    "\n",
    "-   ***price_first***: first published price\n",
    "\n",
    "-   ***details_1_desc***: list of characteristics of the house\n",
    "\n",
    "-   ***details_2_desc***: list of characteristics of the house 2\n",
    "\n",
    "-   ***details_3_desc***: list of characteristics of the house 3\n",
    "\n",
    "-   ***advertiser***: type of advertiser, private, agency..\n",
    "\n",
    "-   ***datetime***: timestamp of scraping date.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finlly we will save our result dataframe into a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data\n",
      "End\n"
     ]
    }
   ],
   "source": [
    "# Saving data\n",
    "now = dt.datetime.now() # current date and time\n",
    "file_name = 'data/raw_data_' + now.strftime(\"%Y%m%d\") + '.csv'\n",
    "\n",
    "print('Saving data')\n",
    "raw_data.to_csv(file_name)\n",
    "\n",
    "print('End')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "be3ee27bd6789846cabdaaf1d172d1b46fe3799ccedac2ef9c2fd3b8a24e7656"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
